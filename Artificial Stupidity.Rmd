---
title: "Artificial Stupidity"
author: "Minghao Du"
date: "10/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list = ls())
gc()

load_pkgs <-
  function(packages = c(),
           basic.packages = c(
             "package:stats",
             "package:graphics",
             "package:grDevices",
             "package:utils",
             "package:datasets",
             "package:methods",
             "package:base"
           )) {
    package.list <-
      search()[ifelse(unlist(gregexpr("package:", search())) == 1, TRUE, FALSE)]
    package.list <- setdiff(package.list, basic.packages)
    if (length(package.list) > 0)
      for (package in package.list)
        detach(package, character.only = TRUE)
    installed_packages <- packages %in% rownames(installed.packages())
    if (any(installed_packages == FALSE)) {
      install.packages(packages[!installed_packages], dependencies = T)
    }
    invisible(lapply(packages, library, character.only = TRUE))
  }


pkg_list <- c(
  "plotly", 
  "tidyverse", 
  "readxl", 
  "GGally", 
  "psych", 
  "janitor", 
  "e1071", 
  "lubridate", 
  "fastDummies", 
  "xgboost", 
  "tensorflow", 
  "keras", 
  "reticulate", 
  "gender", 
  "stringr", 
  "caret", 
  "textstem", 
  "tm", 
  "tidytext", 
  "lhs", 
  "smoof", 
  "mlrMBO", 
  "DiceKriging", 
  "parallel", 
  "mice", 
  "forcats")
load_pkgs(pkg_list)

# install_miniconda()
# https://www.tensorflow.org/install/source_windows 
# https://www.tensorflow.org/install/gpu#hardware_requirements 
# https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow 
# conda_create(envname = "r-reticulate", packages = "tensorflow-gpu", python_version = "3.9")
seed <- 123
saveProcessedData <- F
cores <- detectCores(FALSE, TRUE) - 1
'%!in%' <- function(x,y)!('%in%'(x,y))
use_condaenv("r-reticulate")
```

```{r}
writeSubmit <- function(pred) {
  submissionFile = data.frame(id = scoringData$id, price = pred)
  write.csv(submissionFile, 'submission.csv', row.names = F)
}

rawData <- read_csv('./input/rentlala2021/analysisData.csv')

scoringData <- read_csv('./input/rentlala2021/scoringData.csv')

compare_df_cols(rawData, scoringData) %>%
  filter(rawData != scoringData |
           rawData %>% is.na() | scoringData %>% is.na())

rawData %>% mutate(license = license %>% as.logical()) -> rawData
scoringData  %>% mutate(zipcode = zipcode %>% as.numeric()) -> scoringData

compare_df_cols(rawData, scoringData) %>%
  filter(rawData != scoringData | rawData %>% is.na() | scoringData %>% is.na())
```



```{r}

pricedScoringData <- scoringData %>% mutate(price = -1)

allData <- rawData %>% bind_rows(pricedScoringData) 


extractID <- allData %>% select(id)

allData <- allData %>% select(-id)

numericCols <- allData %>% select(is.numeric) %>% colnames()
boolCols <- allData %>% select(is.logical) %>% colnames()
dateCols <- allData %>% select(is.Date) %>% colnames()
charCols <- allData %>% select(is.character) %>% colnames()
longCharCols <-
  allData[charCols] %>% 
  select(c(name, 
      summary, 
      space, 
      description, 
      neighborhood_overview, 
      notes, 
      transit, 
      access, 
      interaction, 
      house_rules, 
      host_about
  )) %>% 
  colnames()
factorCharCols <- 
  allData[charCols] %>% 
  select(-c(name, 
      summary, 
      space, 
      description, 
      neighborhood_overview, 
      notes, 
      transit, 
      access, 
      interaction, 
      house_rules, 
      host_about
  )) %>% 
  select(-c(
    host_name, 
    host_verifications, 
    host_response_time,
    calendar_updated, 
    host_response_rate, 
    host_acceptance_rate, 
    amenities
  )) %>% 
  colnames()

rateCols <- allData %>% 
  select(c(host_response_rate, host_acceptance_rate)) %>% 
  colnames()


tempCopy <- allData

```



```{r}

allData<- tempCopy

#######
## TODO !!! IMPORTANT !!! 
## 
## handel na values bfore hand !!!
## https://www.kaggle.com/c/predictlala2021/leaderboard 
## 
#######

## TODO: handle outliers 
## TODO: reduce factors 


allData[charCols] <- allData[charCols] %>% mutate_all(funs(enc2utf8(.)))


##### Process zipcode column 

mostCommonZip <- allData %>% 
  select(c(neighbourhood_cleansed, neighbourhood_group_cleansed, city, zipcode)) %>% 
  group_by(neighbourhood_cleansed, zipcode) %>% 
  summarise(count = n()) %>% 
  filter(count == max(count)) %>% 
  ungroup()

getZip <- function(neighbourhood_cleansed){
  mostCommonZip %>% filter(neighbourhood_cleansed == neighbourhood_cleansed) %>% select(zipcode) %>% pull(zipcode)
}

allData <- allData %>% mutate(zipcode = ifelse(is.na(zipcode), getZip(neighbourhood_cleansed), zipcode)) %>% mutate(zipcode = zipcode %>% as.factor())



##### Converte rate cols to rates 

allData[rateCols] <- allData[rateCols] %>% 
  mutate(host_response_rate = gsub("N/A","0%", host_response_rate)) %>% 
  mutate(host_acceptance_rate = gsub("N/A","0%", host_acceptance_rate)) %>% 
  mutate(host_response_rate = gsub("%","", host_response_rate)) %>% 
  mutate(host_acceptance_rate = gsub("%","", host_acceptance_rate)) %>% 
  mutate_all(as.numeric) %>% 
  mutate_each(funs(./100)) %>% 
  replace(is.na(.),0)


temp <- allData


##### Handle/impute NA value for bool and numeric columns

allData <- temp

# allData %>% ggplot(aes(weekly_price)) + geom_density()
# allData %>% ggplot(aes(monthly_price)) + geom_density()

allData[boolCols] <- allData[boolCols] %>% replace(is.na(.),F)

reservedNumCols <- allData %>% select(c(square_feet, weekly_price, monthly_price)) %>% colnames()

allData[numericCols[numericCols %!in% reservedNumCols]] <- 
  allData[numericCols[numericCols %!in% reservedNumCols]] %>% replace(is.na(.),0)

allData <- allData %>% mutate(square_feet = ifelse(square_feet == 0, NA, square_feet))
allData <- allData %>% mutate(weekly_price = ifelse(weekly_price == 0, NA, weekly_price))
allData <- allData %>% mutate(monthly_price = ifelse(monthly_price == 0, NA, monthly_price))

impute <- allData %>% select(-price) %>% select(where(is.numeric)) %>% colnames()
tempData <-
  parlmice(
    allData[impute],
    m = ncol(allData[impute]),
    maxit = 100,
    # n.core = cores, 
    # n.imp.core = ncol(allData[impute]), 
    cluster.seed = seed,
    print = FALSE
  )

allData[impute] <- complete(tempData)

# allData %>% ggplot(aes(weekly_price)) + geom_density()
# allData %>% ggplot(aes(monthly_price)) + geom_density()

temp <- allData


##### Process factor columns 

allData <- temp

# allData <- allData %>% select(-factorCharCols) %>%
#   add_column(allData[factorCharCols] %>%
#   replace(is.na(.),"N/A") %>%
#   dummy_cols(select_columns = factorCharCols, remove_selected_columns = T) %>%
#   clean_names())
#   %>%
#   select_if(~sum(.) != 1))
  
allData[factorCharCols] <- allData[factorCharCols] %>%
  replace(is.na(.), "N/A") %>%
  mutate_all(as.factor)

allData <- allData %>% select(-c(country_code, country, state, market)) 

reduceLevelCols <- allData %>% select(c(host_location, host_neighbourhood, street, neighbourhood_cleansed, city, smart_location, property_type, zipcode)) %>% colnames()

allData[reduceLevelCols] <- allData[reduceLevelCols] %>% mutate_all(funs(fct_lump(., prop = 0.001)))

##### Conject new columns 

## sex 

allData %>% select(host_name) %>% c() -> names 

allData <- allData %>%
  left_join(gender(names$host_name) %>%
              distinct() %>%
              select(c(name, proportion_male, proportion_female)),
            by = c("host_name" = "name"))


## word count 

allData[longCharCols] <- allData[longCharCols] %>% replace(is.na(.), "")

allData <- allData %>%
  bind_cols(allData[longCharCols] %>%
              mutate_each(funs(str_count)) %>%
              rename_all(function(x) paste0(x, "_wcount")))


##### process host verification and amenities 

allData <- allData %>% 
  mutate(host_verifications = gsub("\\[|\\]|\\'|\\,", "", host_verifications)) %>% 
  mutate(vari_count = strsplit(host_verifications, " ") %>% lengths())

vari_list <- allData %>% 
  select(host_verifications) %>% 
  lapply(function(x) unique(trimws(unlist(strsplit(x, " ")))))

for(w in 1:length(vari_list[[1]])) {
  # print(vari_list[[1]][w])
  new <- grepl(pattern = vari_list[[1]][w], x = allData$host_verifications, fixed = TRUE)
  allData[paste(vari_list[[1]][w], "_vari")] <- new
}

allData <- allData %>% select(-host_verifications)

allData <- allData %>% 
  mutate(amenities_count = strsplit(amenities, ",") %>% lengths()) 

amen_list <- allData %>% 
  select(amenities) %>% 
  mutate(amenities = gsub("\\.", "", amenities)) %>% 
  lapply(function(x) unique(trimws(unlist(strsplit(x, ",")))))

for(w in 1:length(amen_list[[1]])) {
  new <- grepl(pattern = amen_list[[1]][w], x = allData$amenities, fixed = TRUE)
  allData[paste(amen_list[[1]][w], "_amen")] <- new
}

allData <- allData %>% select(-amenities) %>% clean_names()


##### process text duration columns 

allData <- allData %>%
  mutate(host_response_time = gsub("within a ", "", host_response_time)) %>%
  mutate(host_response_time = gsub("within an ", "", host_response_time)) %>%
  mutate(host_response_time = gsub("few hours", "12", host_response_time)) %>%
  mutate(host_response_time = gsub("hour", "1", host_response_time)) %>%
  mutate(host_response_time = gsub("a few days or more", "48", host_response_time)) %>%
  mutate(host_response_time = gsub("day", "24", host_response_time)) %>%
  mutate(host_response_time = replace_na(host_response_time, "N/A")) %>% 
  mutate(host_response_time = gsub("N/A", "96", host_response_time)) %>%
  mutate(host_response_time = as.numeric(host_response_time))

allData <- allData %>% 
  mutate(calendar_updated = gsub(" ago", "", calendar_updated)) %>% 
  mutate(calendar_updated = gsub("today", "0", calendar_updated)) %>% 
  mutate(calendar_updated = gsub("yesterday", "1", calendar_updated)) %>% 
  mutate(calendar_updated = case_when(
    grepl("days", calendar_updated) ~ as.numeric(gsub("([0-9]+).*$", "\\1", calendar_updated)) %>% as.character(), 
    grepl("weeks", calendar_updated) ~ as.character(as.numeric(gsub("([0-9]+).*$", "\\1", calendar_updated))*7),
    grepl("months", calendar_updated) ~ as.character(as.numeric(gsub("([0-9]+).*$", "\\1", calendar_updated))*30), 
    grepl("a week", calendar_updated) ~ "7",
    grepl("week", calendar_updated) ~ "7",
    grepl("never", calendar_updated) ~ "3000",
    TRUE ~ as.character(calendar_updated)
  )) %>% 
  mutate(calendar_updated = as.numeric(calendar_updated))


##### process date columns 

allData[dateCols] <- allData %>% 
  select(dateCols) %>% 
  mutate_all(funs(max(., na.rm = TRUE) - .)) %>% 
  mutate_all(as.numeric)



##### Text mining 

prep_fun <- function(x) {
    x %>%
      str_to_lower() %>%
      str_replace_all("[^[:alnum:]]", " ") %>%
      {gsub(patter = "\\d", replace = " ", .)} %>%
      removeWords(stopwords()) %>%
      {gsub(patter = "\\b[A-z]\\b{1}", replace = " ", .)} %>%
      str_replace_all("\\s+", " ") %>%
      lemmatize_strings()
  }

# allData[longCharCols] <- allData[longCharCols] %>%
#   mutate_all(funs(prep_fun(.))) %>%
#   replace(is.na(.), "") 

# textColsAgg <- allData[longCharCols] %>%
#   mutate_all(funs(prep_fun(.))) %>%
#   replace(is.na(.), "") %>%
#   unite(allText) %>%
#   add_column(ID = extractID$id) %>%
#   relocate(ID)
# 
# stemmedText <- textColsAgg %>%
#   unnest_tokens(input = allText, output = word) %>%
#   anti_join(stop_words)


## TODO: generate new numeric cols for long text cols 
## TODO: transform long text cols to numeric value 
```



```{r}

allData <- allData %>% cbind( allData %>% 
  select(where(is.numeric)) %>% 
  select(-price) %>% 
  mutate_all(function(x) x^2) %>% 
  setNames(paste0(names(.), "_2pow")))

numCols <- allData %>% select(is.numeric) %>% colnames()
facCols <- allData %>% select(is.factor) %>% colnames()
boolCols <- allData %>% select(is.logical) %>% colnames()

allData[numCols] <- allData[numCols] %>% 
  mutate_at(vars(-price), funs(scale)) %>%  
  replace(is.na(.),0) 

allData[facCols] <- allData[facCols] %>%
  mutate_all(funs(as.numeric(.)-1))

allData[boolCols] <- allData[boolCols] %>% 
  mutate_all(funs(as.numeric(.)))

remove0var <- function(dat) {
    out <- lapply(dat, function(x) length(unique(x)))
    want <- which(!out > 1)
    name <- unlist(want) %>% names()
    print(name)
    dat %>% select(-name)
}

allData <- allData %>% remove0var() 


# 
# 
# priceTemp <- allData %>% select(price)
# 
# pcaModel <- preProcess(x = allData %>% select(-price) %>% as.matrix(), method = 'pca', thresh = 0.9)
# # pcaModel <- preProcess(x = allData %>% select(-price), method = 'pca', pcaComp = 200)
# 
# allData <- predict(pcaModel, newdata = allData %>% select(-price))
# 
# allData$price <- priceTemp

trainData <- allData %>%
    # add_column(ID = extractID$id) %>%
    # relocate(ID) %>%
    filter(price > -1)
testData <- allData %>%
    # add_column(ID = extractID$id) %>%
    # relocate(ID) %>%
    filter(price == -1) %>% select(-price)

if (saveProcessedData) {
  
  write.csv(trainData,
            file("processedTrainData.csv",encoding="UTF-8"),
            row.names = F)
  write.csv(testData,
            file("processedTestData.csv",encoding="UTF-8"),
            row.names = F)
}


```



```{r}
BoostTrainData <- trainData %>% select(!is.character)

BoostTrainData <- xgb.DMatrix(model.matrix(price ~ ., data = BoostTrainData),
                              label = as.matrix(BoostTrainData %>% select(price)))

obj.fun <- makeSingleObjectiveFunction(
  name = "xgb_cv_bayes",
  fn =   function(x) {
    set.seed(seed)
    cv <- xgb.cv(
      params = list(
        # booster          = "gbtree",
        eta                    = x["eta"],
        max_depth              = x["max_depth"],
        min_child_weight       = x["min_child_weight"],
        gamma                  = x["gamma"],
        subsample              = x["subsample"],
        colsample_bytree       = x["colsample_bytree"],
        max_delta_step         = x["max_delta_step"],
        tweedie_variance_power = x["tweedie_variance_power"],
        objective              = 'reg:tweedie',
        eval_metric            = 'rmse'
      ),
      data = BoostTrainData,
      nround = 7000,
      nthread = cores,
      nfold =  10,
      prediction = FALSE,
      showsd = TRUE,
      early_stopping_rounds = 5,
      verbose = 1,
      print_every_n = 500
    )
    cv$evaluation_log %>% pull(4) %>% min
  },
  par.set = makeParamSet(
    makeNumericParam("eta",                    lower = 0.005, upper = 0.36),
    makeNumericParam("gamma",                  lower = 1,     upper = 8),
    makeIntegerParam("max_depth",              lower = 2,     upper = 15),
    makeIntegerParam("min_child_weight",       lower = 1,     upper = 2000),
    makeNumericParam("subsample",              lower = 0.01,  upper = 1),
    makeNumericParam("colsample_bytree",       lower = 0.01,  upper = 1),
    makeNumericParam("max_delta_step",         lower = 0,     upper = 10),
    makeNumericParam("tweedie_variance_power", lower = 1,     upper = 2)
  ),
  minimize = TRUE
)

do_bayes <-
  function(n_design = NULL,
           opt_steps = NULL,
           of = obj.fun,
           seed = seed) {
    set.seed(seed)
    
    des <- generateDesign(n = n_design,
                          par.set = getParamSet(of),
                          fun = lhs::randomLHS)
    
    control <-
      makeMBOControl() %>% setMBOControlTermination(., iters = opt_steps)
    
    run <- mbo(
      fun = of,
      design = des,
      learner = makeLearner(
        "regr.km",
        predict.type = "se",
        covtype = "matern3_2",
        control = list(trace = FALSE)
      ),
      control = control,
      show.info = TRUE
    )
    
    opt_plot <- run$opt.path$env$path %>%
      mutate(Round = row_number()) %>%
      mutate(type = case_when(Round <= n_design ~ "Design",
                              TRUE ~ "mlrMBO optimization")) %>%
      ggplot(aes(x = Round, y = y, color = type)) +
      geom_point() +
      labs(title = "mlrMBO optimization") +
      ylab("-log(likelihood)")
    
    print(run$x)
    
    return(list(run = run, plot = opt_plot))
  }

# des <- generateDesign(n=15,
#                       par.set = getParamSet(obj.fun),
#                       fun = lhs::randomLHS)

# kable(des, format = "html", digits = 4) %>%
#   kable_styling(font_size = 10) %>%
#   kable_material_dark()

runs <-
  do_bayes(
    n_design = 20,
    of = obj.fun,
    opt_steps = 50,
    seed = seed
  )

plot(runs$run)

best.params <- runs$run$x
# best.params <- list(
#         eta                    = 0.00504, 
#         max_depth              = 8, 
#         min_child_weight       = 1868, 
#         gamma                  = 3.42, 
#         subsample              = 0.996, 
#         colsample_bytree       = 0.253, 
#         max_delta_step         = 2.93, 
#         tweedie_variance_power = 1.34, 
#         objective = 'reg:tweedie',
#         eval_metric = 'rmse'
#       )

set.seed(seed)
optimal.cv <- xgb.cv(
  params = best.params,
  data = BoostTrainData,
  nround = 7000,
  nthread = 12,
  nfold =  10,
  prediction = FALSE,
  showsd = TRUE,
  early_stopping_rounds = 5,
  verbose = 1,
  print_every_n = 100
)

set.seed(seed)
model <-
  xgboost(
    params = best.params,
    data = BoostTrainData,
    nrounds = optimal.cv$best_ntreelimit
  )




# [mbo] 0: eta=0.0435; gamma=6.31; max_depth=12; min_child_weight=1626; subsample=0.714; colsample_bytree=0.439; max_delta_step=2.02; tweedie_variance_power=1.29 : y = 58.3 : 415.4 secs : initdesign
# [mbo] 8: eta=0.0099; gamma=1.18; max_depth=11; min_child_weight=761; subsample=0.707; colsample_bytree=0.637; max_delta_step=6.46; tweedie_variance_power=1.56 : y = 58.2 : 2351.0 secs : infill_cb
### $eta
### [1] 0.00990123
### 
### $gamma
### [1] 1.183968
### 
### $max_depth
### [1] 11
### 
### $min_child_weight
### [1] 761
### 
### $subsample
### [1] 0.7069801
### 
### $colsample_bytree
### [1] 0.6367724
### 
### $max_delta_step
### [1] 6.464198
### 
### $tweedie_variance_power
### [1] 1.562195

# [mbo] 16: eta=0.00533; gamma=2.37; max_depth=12; min_child_weight=1008; subsample=0.87; colsample_bytree=0.715; max_delta_step=6.15; tweedie_variance_power=1.39 : y = 58.2 : 3483.8 secs : infill_cb


# xgb_train_rmse <- NULL
# xgb_test_rmse <- NULL
# round <- NULL
#
# hyper_grid <- expand.grid(max_depth = seq(3, 10, 1), eta = seq(.02, .36, .02), gamma = seq(0, 6, 1))
#
# for(j in 1:nrow(hyper_grid)) {
#   set.seed(seed)
#   m_xgb_untuned <- xgb.cv(
#     data = BoostTrainData,
#     nrounds = 2000,
#     objective = "reg:squarederror",
#     early_stopping_rounds = 3,
#     nfold = 10,
#     max_depth = hyper_grid$max_depth[j],
#     eta = hyper_grid$eta[j],
#     gamma = hyper_grid$gamma[j],
#     verbose = F
#   )
#
#   temp_train_rmse <- m_xgb_untuned$evaluation_log$train_rmse_mean[m_xgb_untuned$best_iteration]
#   temp_test_rmse <- m_xgb_untuned$evaluation_log$test_rmse_mean[m_xgb_untuned$best_iteration]
#
#   cat(j,"\n")
#   cat(hyper_grid$max_depth[j],"\t")
#   cat(hyper_grid$eta[j],"\n")
#   cat(m_xgb_untuned$best_iteration,"\t")
#   cat(temp_train_rmse,"\t")
#   cat(temp_test_rmse,"\n")
#
#   xgb_train_rmse[j] <- temp_train_rmse
#   xgb_test_rmse[j] <- temp_test_rmse
#   round[j] <- m_xgb_untuned$best_iteration
#
#
#   # c(temp_train_rmse, temp_test_rmse)
# }
#
#
#
# set.seed(seed)
# model <-
#   xgboost(
#     data = BoostTrainData %>% select(-price) %>% as.matrix(),
#     label = BoostTrainData %>% select(price) %>% as.matrix(),
#     nrounds = round[which.min(xgb_test_rmse)],
#     objective = "reg:squarederror",
#     early_stopping_rounds = 3,
#     max_depth = hyper_grid[which.min(xgb_test_rmse),]$max_depth,
#     gamma = hyper_grid[which.min(xgb_test_rmse),]$gamma,
#     eta = hyper_grid[which.min(xgb_test_rmse),]$eta
#   )


summary(model)

# xgb.importance(model)

pred <-
  predict(model, model.matrix(price~., testData %>% select(!is.character) %>% mutate(price = -1)) %>% xgb.DMatrix())

writeSubmit(pred)

```




```{r}
NetTrainData <- trainData %>% select(!is.character)

NetTrainX <- NetTrainData %>% select(-price) %>% as.matrix() 
NetTrainY <- NetTrainData %>% select(price) %>% as.matrix()

rmse <- function(y, y_hat){
  sqrt(mean((y - y_hat)^2))
}

inputSize <- dim(NetTrainX)[2] 
offsetSize <- 0.45
dropout <- 0.5
activate <- "relu"

scaleSize <- inputSize * offsetSize

model <- keras_model_sequential() %>%
  layer_dense(units = scaleSize*2, activation = activate, input_shape = dim(NetTrainX)[2]) %>%
  layer_dropout(rate = dropout) %>%
  layer_dense(units = scaleSize*4, activation = activate) %>%
  layer_dropout(rate = dropout) %>%
  layer_dense(units = scaleSize*8, activation = activate) %>%
  layer_dropout(rate = dropout) %>%
  layer_dense(units = scaleSize*4, activation = activate) %>%
  layer_dropout(rate = dropout) %>%
  layer_dense(units = scaleSize*2, activation = activate) %>%
  layer_dropout(rate = dropout) %>%
  layer_dense(units = scaleSize*0.5, activation = activate) %>%
  layer_dense(units = scaleSize*0.25, activation = activate) %>%
  layer_dense(units = 1, activation = activate)

model %>% compile(
   loss = "mse",
   optimizer =  "nadam", 
   metrics = c("mape","mse")
 )
 
model %>% summary()
```

```{r}
set.seed(seed)
model %>% fit(NetTrainX, NetTrainY, epochs = 100, batch_size = 128, validation_split = 0.2,verbose = 2)
 
scores <- model %>% evaluate(NetTrainX, NetTrainY, verbose = 0)
print(scores)
```


```{r}
summary(model)

pred <- predict(model, testData %>% as.matrix())

writeSubmit(pred)
```


```{r}

```



